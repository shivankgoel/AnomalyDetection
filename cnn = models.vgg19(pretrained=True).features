cnn = models.vgg19(pretrained=True).features

import torch
import pickle
import torchvision
from torchvision import transforms, datasets
from torch.autograd import Variable
import torch.nn as nn
import torch.optim as optim
import torch.optim.lr_scheduler as lr_scheduler
import time
import torch.nn.functional as F

num_classes = 35

is_dropout = True
is_relu = True
optimization = "Paper" # can be "SGD", "SGD_M", "ADAM"

val_acc_list = []
test_acc_list = []
train_acc_list= []

class Net(nn.Module):
	def __init__(self):
		global num_classes
		super(Net, self).__init__()
		self.conv1 = nn.Conv2d(3, 96, 11, stride = 4, padding = 3)
		self.conv2 = nn.Conv2d(96, 256, 5, stride = 1, padding = 2, groups = 2)
		self.conv3 = nn.Conv2d(256, 384, 3, padding = 1)
		self.conv4 = nn.Conv2d(384, 384, 3, padding = 1, groups = 2)
		self.conv5 = nn.Conv2d(384, 256, 3, padding = 1, groups = 2)
		self.fc1   = nn.Linear(256*6*6, 256)
		self.fc2   = nn.Linear(256, 128)
		self.fc3   = nn.Linear(128, num_classes)
			
	def forward(self, x):
    		global is_dropout, is_relu

                drop = F.dropout

                if(is_relu):
                        activation = F.relu
                else:
                        activation = F.tanh

	        x = F.max_pool2d(activation(self.conv1(x)), 3, stride = 2)
        	x = F.max_pool2d(activation(self.conv2(x)), 3, stride = 2)
       		x = activation(self.conv3(x))
		x = activation(self.conv4(x))
	        x = F.max_pool2d(activation(self.conv5(x)), 3, stride = 2)

	        x = x.view(-1, 256*6*6)

	        x = activation(self.fc1(x))
	        if(not is_dropout):
	        	x = drop(x)
		x = activation(self.fc2(x))
	        if(not is_dropout):
        		x = drop(x)
	        x = self.fc3(x)
        	return x

def take_input(folder):
	data_transform = transforms.Compose([
	    transforms.Scale(256),
	    transforms.CenterCrop(256),
	    transforms.RandomCrop(224),
	    transforms.RandomHorizontalFlip(),
	    transforms.ToTensor()
	])
	hymenoptera_dataset = datasets.ImageFolder(root=folder,
	                                       transform=data_transform)
	dataset_loader = torch.utils.data.DataLoader(hymenoptera_dataset,
	                                         batch_size=128, shuffle=True, num_workers=8)
	return dataset_loader


def train(alexnet, scheduler, optimizer, train_dataset_loader, val_dataset_loader, test_dataset_loader):
	global val_acc_list,  train_acc_list
	alexnet.train()
	for epochs in range(75):
		print("epoch %s", epochs)
		correct = 0
		total = 0
		start_time = time.time()
		for i, data in enumerate(train_dataset_loader, 0):
			if(i%100 == 99):
				print(i)
			loss = 0.0
			inputs, labels = data
			labels1 = labels
			inputs, labels = Variable(inputs.cuda()), Variable(labels.cuda())
			optimizer.zero_grad()
			outputs = alexnet(inputs)
	        	loss = criterion(outputs, labels)
        		loss.backward()
        		optimizer.step()
		
			_, predicted = torch.max(outputs.data, 1)
	                total += labels.size(0)
        	        correct += (predicted.cpu() == labels1).sum()
	       	val_acc = predict_test(alexnet, val_dataset_loader)
		train_acc = 100.0 * correct/total
	        scheduler.step(val_acc)
      	 	
	        val_acc_list.append(val_acc)
        	train_acc_list.append(train_acc)

	        print("Epoch id = ", epochs, " Time = ", (time.time() - start_time))
        	print("Epoch id = ", epochs, " Val accuracy = ", val_acc)
        	print("Epoch id = ", epochs, " Train accuracy = ", train_acc)   	

def predict_test(alexnet, test_dataset_loader):
	alexnet.eval()
	correct = 0
	total = 0
	for data in test_dataset_loader:
		images, labels = data
		outputs = alexnet(Variable(images.cuda()))
		_, predicted = torch.max(outputs.data, 1)
    		total += labels.size(0)
		correct += (predicted.cpu() == labels).sum()
	print ("Accuracy is %f, %s, %s", 100.0*correct/total, correct, total)
	return 100.0*correct/total


train_dataset_loader = take_input('/home/cse/btech/cs1140091/COL865/dataset/train/')
test_dataset_loader = take_input('/home/cse/btech/cs1140091/COL865/dataset/test/')
val_dataset_loader = take_input('/home/cse/btech/cs1140091/COL865/dataset/validation/')


alexnet = Net().cuda()
print(alexnet)
params = list(alexnet.parameters())
print(len(params))
print(params[0].size())

criterion = nn.CrossEntropyLoss()

if(optimization == "SGD"):
	optimizer = optim.SGD(alexnet.parameters(), lr=0.01)
elif(optimization == "SGD_M"):
	optimizer = optim.SGD(alexnet.parameters(), lr=0.01,momentum=0.9)
elif(optimization == "ADAM"):
	optimizer = optim.Adam(alexnet.parameters(), lr=0.0001)
else:
	optimizer = optim.SGD(alexnet.parameters(), lr=0.01, momentum=0.9, weight_decay = 0.0005)

# optimizer = optim.SGD(alexnet.parameters(), lr=0.001, momentum=0.9)

new_lr_scheduler = lr_scheduler.ReduceLROnPlateau(optimizer, 'max')

train_start = time.time()
train(alexnet, new_lr_scheduler, optimizer, train_dataset_loader, val_dataset_loader, test_dataset_loader)
train_end = time.time()

print("Time to train = ", (train_end-train_start))

test_start = time.time()
test_acc = predict_test(alexnet, test_dataset_loader)
print("Time to train = ", (time.time()-test_start))

print("The test accuracy is = ", test_acc)

train_error_file = open("/home/cse/btech/cs1140091/COL865/org/train_error_org.txt", 'w')
val_error_file = open("/home/cse/btech/cs1140091/COL865/org/val_error.txt_org", 'w')
pickle.dump(train_acc_list, train_error_file)
pickle.dump(val_acc_list, val_error_file)
torch.save(alexnet, "/home/cse/btech/cs1140091/COL865/org/model.pt")
